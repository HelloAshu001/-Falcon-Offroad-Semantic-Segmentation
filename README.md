# ğŸ† Falcon Offroad Semantic Segmentation

Production-ready **Semantic Segmentation pipeline** for Falcon off-road environments using **DeepLabV3+**.  
Built for the **Duality AI Falcon Hackathon**, with strict dataset separation, automated checkpoints, IoU logging, batch inference, and a Streamlit dashboard.

---

## ğŸš€ Key Features

- DeepLabV3+ with ResNet-101 backbone
- Pixel-wise semantic segmentation
- Strict **train / val / test** separation
- Automatic checkpoint saving (best & latest)
- IoU-based validation
- Batch prediction on test images
- Streamlit-based training dashboard

---

<h2>ğŸ“ Project Structure</h2>

<pre>
Falcon-Offroad-Semantic-Segmentation/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py                 # Streamlit dashboard
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml             # Configuration file
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ images/             # From train/color
â”‚   â”‚   â””â”€â”€ masks/              # From train/segmentation
â”‚   â”‚
â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ images/             # From val/color
â”‚   â”‚   â””â”€â”€ masks/              # From val/segmentation
â”‚   â”‚
â”‚   â””â”€â”€ testImages/
â”‚       â””â”€â”€ images/             # From testImages/color ONLY
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ deeplabv3plus.py        # Model architecture
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset.py              # Dataset loader
â”‚   â”œâ”€â”€ trainer.py              # Training logic
â”‚   â”œâ”€â”€ metrics.py              # IoU calculation
â”‚   â”œâ”€â”€ logger.py               # Training logs
â”‚   â””â”€â”€ checkpoint.py           # Model checkpoints
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ predict_all.py          # Batch inference
â”‚
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ checkpoints/            # Saved models
â”‚   â”œâ”€â”€ logs/                   # Training logs
â”‚   â””â”€â”€ results/                # Predictions
â”‚
â”œâ”€â”€ train.py                    # Training entry point
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md
</pre>
## ğŸ“Š Dataset Explanation

The Falcon dataset follows this naming convention:

| Folder Name     | Description                     |
|-----------------|---------------------------------|
| `color/`        | RGB input images                |
| `segmentation/` | Pixel-wise ground truth masks   |

### Dataset Usage Rules

- **Training & Validation**
  - Use both `color` and `segmentation`
- **Test Dataset**
  - Use **ONLY** `color`
  - âŒ Never use test segmentation for training

---

## â–¶ï¸ Step-by-Step Instructions to Run and Test the Model

Follow the steps below to train the model and test it on the Falcon off-road dataset.

---

### Step 1: Clone the Repository

```bash
git clone https://github.com/Tushar7902/Falcon-Offroad-Semantic-Segmentation.git
cd Falcon-Offroad-Semantic-Segmentation
```
### Step 2: Set Up the Environment

Ensure Python 3.11 is installed. Create and activate the environment, then install dependencies:
```bash
conda create -n falcon python=3.11 -y
conda activate falcon
pip install -r requirements.txt
```

### Step 3: Prepare the Dataset
Place the dataset in the following structure:
```text
train/color        â†’ data/train/images
train/segmentation â†’ data/train/masks

val/color          â†’ data/val/images
val/segmentation   â†’ data/val/masks

testImages/color   â†’ data/testImages/images
```
âš ï¸ Test segmentation masks must not be used for training.

### Step 4: Train the Model
Run the training script:
```bash
python train.py
```
Outputs generated:

- Best model checkpoint: runs/checkpoints/best_model.pth

- Training logs: runs/logs/training_log.csv

### Step 5: Test the Model (Inference)
Run inference on the test images:
```bash
python scripts/predict_all.py
```
Outputs generated:

- Predicted segmentation masks saved in: runs/results/
### Step 6: Verify the Outputs
- Ensure output masks are generated for each test image.

- Pixel values in output masks correspond to semantic classes.

- Higher validation IoU indicates better segmentation performance.
## Optional: Visualize Training Progress
```bash
streamlit run app/main.py
```
## ğŸ” Reproducing the Final Results

This section explains how to reproduce the final results using the trained model checkpoint.

---

### Step 1: Set Up the Environment

Ensure the environment and dependencies are installed as described in the **Environment Setup** section.

```bash
conda activate falcon
```
## Step 2: Prepare the Dataset
Verify that the dataset is placed correctly:
```bash
data/
â”œâ”€â”€ train/images
â”œâ”€â”€ train/masks
â”œâ”€â”€ val/images
â”œâ”€â”€ val/masks
â””â”€â”€ testImages/images
```

âš ï¸ Only test images are used during this step.
Test segmentation masks are not used.
## Step 3: Use the Trained Model
After training, the best-performing model is saved automatically at:
```bash
runs/checkpoints/best_model.pth
```
This checkpoint is selected based on validation IoU.
## Step 4: Run Inference on Test Images
Generate the final segmentation results by running:
```bash
python scripts/predict_all.py
```
## Step 5: Locate the Final Outputs
The reproduced results are saved in:
```bash
runs/results/
```
Each output file corresponds to one input test image and contains pixel-wise semantic predictions.
## Step 6: Result Interpretation
- Output images represent predicted segmentation masks.

- Each pixel value corresponds to a semantic class defined by the dataset.

- Model performance is evaluated using Intersection over Union (IoU) on the validation set.

- Final test evaluation is performed by the challenge organizers using hidden ground truth.
## Reproducibility Statement
- All results are reproducible by following the steps above.

- No external data or test annotations are used during training or inference.

- The model behavior is deterministic given the same dataset and configuration.
## ğŸ› ï¸ Environment & Dependency Requirements

The project was developed and tested using the following environment configuration.

### System Requirements

- **Operating System:** Linux / macOS / Windows  
- **Python Version:** 3.11 (tested and supported)  
- **Framework:** PyTorch  
- **GPU:** Optional (CUDA-enabled GPU recommended for faster training)

> âš ï¸ Python versions **3.12 and above** are not fully supported by PyTorch at the time of development and may cause installation or build issues.

---

### Environment Setup

Create and activate a dedicated Conda environment and install all required dependencies using the commands below:

```bash
conda create -n falcon python=3.11 -y
conda activate falcon
pip install -r requirements.txt
```
## Python Dependencies
All required Python packages are listed in the requirements.txt file, including but not limited to:
- torch
- torchvision
- numpy
- opencv-python
- matplotlib
- albumentations
- segmentation-models-pytorch
- streamlit
- Installing dependencies from requirements.txt ensures a consistent and reproducible environment across different systems.
## ğŸ“Š Notes on Expected Outputs and How to Interpret Them

This section explains the outputs generated by the model and how to interpret them correctly.

---

### ğŸ“ Output Directory Structure

After training and inference, the following output directories are created automatically:

```text
runs/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pth      # Best model based on validation IoU
â”‚   â””â”€â”€ latest.pth          # Most recent training checkpoint
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_log.csv    # Training loss and IoU per epoch
â”‚
â””â”€â”€ results/
    â””â”€â”€ *.png               # Predicted segmentation masks for test images
```
## ğŸ§  Model Checkpoints
- best_model.pth

  - Represents the model with the highest validation IoU.

  - Used to generate final test predictions.

- latest.pth

  - Stores the most recent training state.

  - Useful for resuming training.
## ğŸ“ˆ Training Logs
The file training_log.csv contains:
- Epoch number
- Training loss
- Validation IoU
Interpretation:
- Decreasing loss indicates improved learning.
- Increasing IoU indicates better segmentation performance.
- The epoch with the highest IoU corresponds to best_model.pth.
## ğŸ–¼ï¸ Predicted Segmentation Outputs
- Predicted masks are saved in:
```bash
runs/results/
```
- Each output file corresponds to a test input image.
- Output images are single-channel segmentation masks.
Pixel Interpretation:
- Each pixel value represents a semantic class.
- Pixel values map directly to class labels defined by the Falcon dataset.
- Regions with the same pixel value belong to the same class.
## ğŸ“ Evaluation Metric
- Metric Used: Intersection over Union (IoU)
- IoU is calculated on the validation set during training.
- Final test IoU is computed by the challenge organizers using hidden ground truth.
- Higher IoU values indicate better segmentation quality.
## âœ… Expected Results Summary
- Checkpoints saved successfully after training
- Training logs available for performance analysis
- Segmentation masks generated for all test images
- Outputs reproducible using the provided steps
## ğŸ† Interpretation Guidelines
- Well-segmented regions align closely with scene objects.
- Sharp boundaries indicate good class separation.
- Misclassified regions may indicate class imbalance or visual similarity.
- All outputs can be reproduced by following the steps outlined in this README.
## ğŸ“Œ Results Summary

The model successfully generates pixel-wise segmentation masks for all test images using the best checkpoint selected based on validation IoU.

